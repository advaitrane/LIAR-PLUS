{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description\n",
    "\n",
    "This notebook contains\n",
    "- implementation and training of binary classification using a slightly deeper parallel Bi-LSTM for the condition SJ\n",
    "\n",
    "## Model description\n",
    "### Binary - model_sj\n",
    "- tokenize the texts for the statements and the justification\n",
    "- embedding layer using glove embeddings for each, statements and justifications\n",
    "- two parallel Bi-LSTM layers, one for the statements and one for the justifications\n",
    "- a concatenate layer to merge the result of the two Bi-LSTM layers\n",
    "- two dense layers ending with a softmax activation with 2 output units\n",
    "\n",
    "## Result\n",
    "### Binary - model_sj\n",
    "- Val accuracy = 60.59%\n",
    "- Test accuracy = 57.85%\n",
    "\n",
    "## Weights file\n",
    "### Binary - model_sj\n",
    "- model_d_sj_weights_1.h5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = \"dataset\"\n",
    "\n",
    "train_data_file = os.path.join(dataset_dir, \"train2.tsv\")\n",
    "test_data_file = os.path.join(dataset_dir, \"test2.tsv\")\n",
    "val_data_file = os.path.join(dataset_dir, \"val2.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# column names are taken from the readme.md of the LIAR-PLUS github repo -\n",
    "# link to repo - https://github.com/Tariq60/LIAR-PLUS\n",
    "\n",
    "col_names = [\"id\", \"label\", \"statement\", \"subject\", \"speaker\", \"speaker_job\", \"state_info\", \"party\", \\\n",
    "             \"barely_true\", \"false\", \"half_true\", \"mostly_true\", \"pants_on_fire\", \"context\", \"ex_just\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(train_data_file, sep = '\\t', header = None, names = col_names,)# na_values = [\"NaN\"], na_filter = True)\n",
    "test_data = pd.read_csv(test_data_file, sep = '\\t', header = None, names = col_names)\n",
    "val_data = pd.read_csv(val_data_file, sep = '\\t', header = None, names = col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/advaitrane/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/advaitrane/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/advaitrane/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/advaitrane/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/advaitrane/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/advaitrane/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, LSTM, Embedding, Input, Bidirectional\n",
    "from keras.initializers import Constant\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using glove embeddings, as mentioned in the paper. Reference taken from keras documentation on using pretrained word embeddings\n",
    "# link to reference - https://keras.io/examples/pretrained_word_embeddings/\n",
    "# link to download glove embeddings - https://nlp.stanford.edu/projects/glove/\n",
    "\n",
    "glove_file = os.path.join(\"glove\", \"glove.6B.100d.txt\")\n",
    "max_no_of_words = 20000\n",
    "embeddings_dim = 100\n",
    "max_len_seq = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "with open(glove_file) as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizing the statements and the justifications\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_no_of_words)\n",
    "\n",
    "tokenizer.fit_on_texts(list(train_data[\"statement\"]))\n",
    "tokenizer.fit_on_texts(list(train_data[\"ex_just\"].replace(np.nan, \"\", regex = True)))\n",
    "\n",
    "train_stm_sequences = tokenizer.texts_to_sequences(list(train_data[\"statement\"]))\n",
    "train_just_sequences = tokenizer.texts_to_sequences(list(train_data[\"ex_just\"].replace(np.nan, \"\", regex = True)))\n",
    "\n",
    "val_stm_sequences = tokenizer.texts_to_sequences(list(val_data[\"statement\"]))\n",
    "val_just_sequences = tokenizer.texts_to_sequences(list(val_data[\"ex_just\"].replace(np.nan, \"\", regex = True)))\n",
    "\n",
    "test_stm_sequences = tokenizer.texts_to_sequences(list(test_data[\"statement\"]))\n",
    "test_just_sequences = tokenizer.texts_to_sequences(list(test_data[\"ex_just\"].replace(np.nan, \"\", regex = True)))\n",
    "\n",
    "train_stm_seq = np.array(pad_sequences(train_stm_sequences, maxlen = max_len_seq))\n",
    "train_just_seq = np.array(pad_sequences(train_just_sequences, maxlen = max_len_seq))\n",
    "\n",
    "val_stm_seq = np.array(pad_sequences(val_stm_sequences, maxlen = max_len_seq))\n",
    "val_just_seq = np.array(pad_sequences(val_just_sequences, maxlen = max_len_seq))\n",
    "\n",
    "test_stm_seq = np.array(pad_sequences(test_stm_sequences, maxlen = max_len_seq))\n",
    "test_just_seq = np.array(pad_sequences(test_just_sequences, maxlen = max_len_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10240, 1000)\n",
      "(10240, 1000)\n",
      "(1267, 1000)\n",
      "(1267, 1000)\n",
      "(1284, 1000)\n",
      "(1284, 1000)\n"
     ]
    }
   ],
   "source": [
    "print(train_stm_seq.shape)\n",
    "print(train_just_seq.shape)\n",
    "print(test_stm_seq.shape)\n",
    "print(test_just_seq.shape)\n",
    "print(val_stm_seq.shape)\n",
    "print(val_just_seq.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = min(max_no_of_words, len(tokenizer.word_index)) + 1 # add 1 tokenizer index starts from 1\n",
    "embedding_matrix = np.zeros((num_words, embeddings_dim)) \n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if i > num_words-1:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word) # to avoid KeyError exception\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros as .get will return None\n",
    "        embedding_matrix[i] = embedding_vector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_val = {\"pants-fire\":0, \"false\":0, \"barely-true\":0, \"half-true\":1, \"mostly-true\":1, \"true\":1}\n",
    "train_tf = np.array(list(map(lambda l: tf_val[l], list(train_data[\"label\"]))))\n",
    "train_cat_tf = to_categorical(train_tf)\n",
    "\n",
    "val_tf = np.array(list(map(lambda l: tf_val[l], list(val_data[\"label\"]))))\n",
    "val_cat_tf = to_categorical(val_tf)\n",
    "\n",
    "test_tf = np.array(list(map(lambda l: tf_val[l], list(test_data[\"label\"]))))\n",
    "test_cat_tf = to_categorical(test_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/advaitrane/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import concatenate\n",
    "\n",
    "stm_inp = Input(shape = (max_len_seq, ), dtype = 'int32')\n",
    "stm_x = Embedding(num_words, embeddings_dim, embeddings_initializer = Constant(embedding_matrix),\n",
    "                    input_length = max_len_seq, trainable = False)(stm_inp)\n",
    "stm_x = Bidirectional(LSTM(64))(stm_x)\n",
    "\n",
    "just_inp = Input(shape = (max_len_seq,), dtype = \"int32\")\n",
    "just_x = Embedding(num_words, embeddings_dim, embeddings_initializer = Constant(embedding_matrix),\n",
    "                    input_length = max_len_seq, trainable = False)(just_inp)\n",
    "just_x = Bidirectional(LSTM(64))(just_x)\n",
    "\n",
    "x_sj = concatenate([stm_x, just_x])\n",
    "x_sj = Dense(64, activation = 'tanh')(x_sj)\n",
    "c_sj = Dense(2, activation = 'softmax')(x_sj)\n",
    "\n",
    "model_sj = Model(inputs = [stm_inp, just_inp], outputs = c_sj)\n",
    "\n",
    "model_sj.compile(loss = \"categorical_crossentropy\", optimizer = \"adam\", metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/advaitrane/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 10240 samples, validate on 1284 samples\n",
      "Epoch 1/10\n",
      "   32/10240 [..............................] - ETA: 35:02 - loss: 0.7749 - acc: 0.4375"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-30ecc14e49cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_sj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_stm_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_just_seq\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_cat_tf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval_stm_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_just_seq\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_cat_tf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_sj.fit([train_stm_seq, train_just_seq], train_cat_tf, batch_size = 32, epochs = 10, verbose = 1, validation_data = ([val_stm_seq, val_just_seq], val_cat_tf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1267/1267 [==============================] - 21s 17ms/step\n",
      "test accuracy = 0.5785319652252529\n",
      "[0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
      "0           true\n",
      "1          false\n",
      "2          false\n",
      "3      half-true\n",
      "4     pants-fire\n",
      "5           true\n",
      "6           true\n",
      "7    barely-true\n",
      "8           true\n",
      "9    barely-true\n",
      "Name: label, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(\"test accuracy = {}\".format(model_sj.evaluate([test_stm_seq, test_just_seq], test_cat_tf)[1]))\n",
    "\n",
    "pred_prob_sj = model_sj.predict([test_stm_seq, test_just_seq])\n",
    "print(list(np.argmax(pred_prob_sj[:10], axis = 1)))\n",
    "print(test_data[\"label\"].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sj.save_weights(\"model_d_sj_weights_1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
