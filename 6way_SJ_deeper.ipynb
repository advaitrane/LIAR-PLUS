{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description\n",
    "This notebook contains\n",
    "- implementation and training of binary classification using a slightly deeper parallel Bi-LSTM for the condition SJ\n",
    "\n",
    "## Model description\n",
    "### 6 way - model_sj_6\n",
    "- tokenize the texts for the statements and the justification\n",
    "- embedding layer using glove embeddings for each, statements and justifications\n",
    "- two parallel Bi-LSTM layers, one for the statements and one for the justifications\n",
    "- a concatenate layer to merge the result of the two Bi-LSTM layers\n",
    "- two dense layers ending with a softmax activation with 6 output units\n",
    "\n",
    "## Results\n",
    "### 6 way - model_sj_6\n",
    "- Val accuracy = 24.38%\n",
    "- Test accuracy = 21.94%\n",
    "\n",
    "## Weights file\n",
    "### 6 way - model_sj_6\n",
    "- model_d_sj_6_weights_1.h5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = \"dataset\"\n",
    "\n",
    "train_data_file = os.path.join(dataset_dir, \"train2.tsv\")\n",
    "test_data_file = os.path.join(dataset_dir, \"test2.tsv\")\n",
    "val_data_file = os.path.join(dataset_dir, \"val2.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# column names are taken from the readme.md of the LIAR-PLUS github repo -\n",
    "# link to repo - https://github.com/Tariq60/LIAR-PLUS\n",
    "\n",
    "col_names = [\"id\", \"label\", \"statement\", \"subject\", \"speaker\", \"speaker_job\", \"state_info\", \"party\", \\\n",
    "             \"barely_true\", \"false\", \"half_true\", \"mostly_true\", \"pants_on_fire\", \"context\", \"ex_just\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(train_data_file, sep = '\\t', header = None, names = col_names,)# na_values = [\"NaN\"], na_filter = True)\n",
    "test_data = pd.read_csv(test_data_file, sep = '\\t', header = None, names = col_names)\n",
    "val_data = pd.read_csv(val_data_file, sep = '\\t', header = None, names = col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/advaitrane/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/advaitrane/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/advaitrane/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/advaitrane/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/advaitrane/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/advaitrane/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, LSTM, Embedding, Input, Bidirectional\n",
    "from keras.initializers import Constant\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using glove embeddings, as mentioned in the paper. Reference taken from keras documentation on using pretrained word embeddings\n",
    "# link to reference - https://keras.io/examples/pretrained_word_embeddings/\n",
    "# link to download glove embeddings - https://nlp.stanford.edu/projects/glove/\n",
    "\n",
    "glove_file = os.path.join(\"glove\", \"glove.6B.100d.txt\")\n",
    "max_no_of_words = 20000\n",
    "embeddings_dim = 100\n",
    "max_len_seq = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "with open(glove_file) as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=max_no_of_words)\n",
    "\n",
    "tokenizer.fit_on_texts(list(train_data[\"statement\"]))\n",
    "tokenizer.fit_on_texts(list(train_data[\"ex_just\"].replace(np.nan, \"\", regex = True)))\n",
    "\n",
    "train_stm_sequences = tokenizer.texts_to_sequences(list(train_data[\"statement\"]))\n",
    "train_just_sequences = tokenizer.texts_to_sequences(list(train_data[\"ex_just\"].replace(np.nan, \"\", regex = True)))\n",
    "\n",
    "val_stm_sequences = tokenizer.texts_to_sequences(list(val_data[\"statement\"]))\n",
    "val_just_sequences = tokenizer.texts_to_sequences(list(val_data[\"ex_just\"].replace(np.nan, \"\", regex = True)))\n",
    "\n",
    "test_stm_sequences = tokenizer.texts_to_sequences(list(test_data[\"statement\"]))\n",
    "test_just_sequences = tokenizer.texts_to_sequences(list(test_data[\"ex_just\"].replace(np.nan, \"\", regex = True)))\n",
    "\n",
    "train_stm_seq = np.array(pad_sequences(train_stm_sequences, maxlen = max_len_seq))\n",
    "train_just_seq = np.array(pad_sequences(train_just_sequences, maxlen = max_len_seq))\n",
    "\n",
    "val_stm_seq = np.array(pad_sequences(val_stm_sequences, maxlen = max_len_seq))\n",
    "val_just_seq = np.array(pad_sequences(val_just_sequences, maxlen = max_len_seq))\n",
    "\n",
    "test_stm_seq = np.array(pad_sequences(test_stm_sequences, maxlen = max_len_seq))\n",
    "test_just_seq = np.array(pad_sequences(test_just_sequences, maxlen = max_len_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = min(max_no_of_words, len(tokenizer.word_index)) + 1 # add 1 tokenizer index starts from 1\n",
    "embedding_matrix = np.zeros((num_words, embeddings_dim)) \n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if i > num_words-1:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word) # to avoid KeyError exception\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros as .get will return None\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "six_val = {\"pants-fire\":0, \"false\":1, \"barely-true\":2, \"half-true\":3, \"mostly-true\":4, \"true\":5}\n",
    "rev_six_val = dict(map(reversed, six_val.items()))\n",
    "\n",
    "train_sj_6 = np.array(list(map(lambda l: six_val[l], list(train_data[\"label\"]))))\n",
    "train_cat_sj_6 = to_categorical(train_sj_6)\n",
    "\n",
    "val_sj_6 = np.array(list(map(lambda l: six_val[l], list(val_data[\"label\"]))))\n",
    "val_cat_sj_6 = to_categorical(val_sj_6)\n",
    "\n",
    "test_sj_6 = np.array(list(map(lambda l: six_val[l], list(test_data[\"label\"]))))\n",
    "test_cat_sj_6 = to_categorical(test_sj_6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/advaitrane/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import concatenate\n",
    "\n",
    "stm_inp_6 = Input(shape = (max_len_seq, ), dtype = 'int32')\n",
    "stm_x_6 = Embedding(num_words, embeddings_dim, embeddings_initializer = Constant(embedding_matrix),\n",
    "                    input_length = max_len_seq, trainable = False)(stm_inp_6)\n",
    "stm_x_6 = Bidirectional(LSTM(64))(stm_x_6)\n",
    "\n",
    "just_inp_6 = Input(shape = (max_len_seq,), dtype = \"int32\")\n",
    "just_x_6 = Embedding(num_words, embeddings_dim, embeddings_initializer = Constant(embedding_matrix),\n",
    "                    input_length = max_len_seq, trainable = False)(just_inp_6)\n",
    "just_x_6 = Bidirectional(LSTM(64))(just_x_6)\n",
    "\n",
    "x_sj_6 = concatenate([stm_x_6, just_x_6])\n",
    "x_sj_6 = Dense(64, activation = 'tanh')(x_sj_6)\n",
    "c_sj_6 = Dense(6, activation = 'softmax')(x_sj_6)\n",
    "\n",
    "model_sj_6 = Model(inputs = [stm_inp_6, just_inp_6], outputs = c_sj_6)\n",
    "\n",
    "model_sj_6.compile(loss = \"categorical_crossentropy\", optimizer = \"adam\", metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/advaitrane/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 10240 samples, validate on 1284 samples\n",
      "Epoch 1/10\n",
      "10240/10240 [==============================] - 2037s 199ms/step - loss: 1.7480 - acc: 0.2178 - val_loss: 1.7072 - val_acc: 0.2523\n",
      "Epoch 2/10\n",
      "10240/10240 [==============================] - 1905s 186ms/step - loss: 1.7057 - acc: 0.2580 - val_loss: 1.7064 - val_acc: 0.2523\n",
      "Epoch 3/10\n",
      "10240/10240 [==============================] - 1699s 166ms/step - loss: 1.6822 - acc: 0.2721 - val_loss: 1.7045 - val_acc: 0.2539\n",
      "Epoch 4/10\n",
      "10240/10240 [==============================] - 1212s 118ms/step - loss: 1.6493 - acc: 0.2999 - val_loss: 1.6993 - val_acc: 0.2609\n",
      "Epoch 5/10\n",
      "10240/10240 [==============================] - 1054s 103ms/step - loss: 1.5947 - acc: 0.3337 - val_loss: 1.7282 - val_acc: 0.2508\n",
      "Epoch 6/10\n",
      "10240/10240 [==============================] - 634s 62ms/step - loss: 1.5268 - acc: 0.3782 - val_loss: 1.7624 - val_acc: 0.2632\n",
      "Epoch 7/10\n",
      "10240/10240 [==============================] - 630s 62ms/step - loss: 1.4267 - acc: 0.4342 - val_loss: 1.8749 - val_acc: 0.2399\n",
      "Epoch 8/10\n",
      "10240/10240 [==============================] - 629s 61ms/step - loss: 1.3098 - acc: 0.4873 - val_loss: 1.9419 - val_acc: 0.2500\n",
      "Epoch 9/10\n",
      "10240/10240 [==============================] - 654s 64ms/step - loss: 1.1592 - acc: 0.5616 - val_loss: 2.0979 - val_acc: 0.2383\n",
      "Epoch 10/10\n",
      "10240/10240 [==============================] - 1106s 108ms/step - loss: 0.9934 - acc: 0.6288 - val_loss: 2.3595 - val_acc: 0.2438\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12cdfdd30>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_sj_6.fit([train_stm_seq, train_just_seq], train_cat_sj_6, batch_size = 32, epochs = 10, verbose = 1, validation_data = ([val_stm_seq, val_just_seq], val_cat_sj_6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1267/1267 [==============================] - 22s 17ms/step\n",
      "test accuracy = 0.21941594317872973\n",
      "['pants-fire', 'false', 'barely-true', 'false', 'false', 'barely-true', 'barely-true', 'false', 'false', 'barely-true']\n",
      "0           true\n",
      "1          false\n",
      "2          false\n",
      "3      half-true\n",
      "4     pants-fire\n",
      "5           true\n",
      "6           true\n",
      "7    barely-true\n",
      "8           true\n",
      "9    barely-true\n",
      "Name: label, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(\"test accuracy = {}\".format(model_sj_6.evaluate([test_stm_seq, test_just_seq], test_cat_sj_6)[1]))\n",
    "\n",
    "pred_prob_sj_6 = model_sj_6.predict([test_stm_seq, test_just_seq])\n",
    "print(list(map(lambda r: rev_six_val[r], list(np.argmax(pred_prob_sj_6[:10], axis = 1)))))\n",
    "print(test_data[\"label\"].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sj_6.save_weights(\"model_d_sj_6_weights_1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
